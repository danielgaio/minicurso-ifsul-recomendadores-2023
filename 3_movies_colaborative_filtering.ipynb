{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux class to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from collections import defaultdict\n",
    "\n",
    "class MovieLens:\n",
    "    movieID_to_name = {}\n",
    "    name_to_movieID = {}\n",
    "    ratings_path = 'ml-latest-small/ratings.csv'\n",
    "    movies_path = 'ml-latest-small/movies.csv'\n",
    "    \n",
    "    def load_movie_lens_latest_small(self):\n",
    "        ratings_dataset = 0\n",
    "        self.movieID_to_name = {}\n",
    "        self.name_to_movieID = {}\n",
    "\n",
    "        \"\"\"\" \n",
    "        The `Reader` class from the `surprise` library in Python, is a library for building and analyzing recommender systems.\n",
    "\n",
    "        Here's what each part does:\n",
    "\n",
    "        - `Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)`: This initializes a new `Reader` object with a specific line format and separator.\n",
    "            - `line_format='user item rating timestamp'`: This tells the `Reader` that each line in the data file will be in the format of 'user item rating timestamp'. It's a space-separated string where each word represents the order of the data in the file.\n",
    "            - `sep=','`: This tells the `Reader` that the data in the file is separated by commas.\n",
    "            - `skip_lines=1`: This tells the `Reader` to skip the first line of the file, which is typically the header.\n",
    "\n",
    "        This line of code is typically used when you're about to load a dataset using the `Dataset.load_from_file()` or `Dataset.load_from_df()` methods from the `surprise` library. The `Reader` object tells these methods how to interpret the data file or DataFrame.\n",
    "        \"\"\"\n",
    "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "        # Load ratings dataset\n",
    "        ratings_dataset = Dataset.load_from_file(self.ratings_path, reader=reader)\n",
    "\n",
    "        # Create mappings from movie name to movie id and vice versa\n",
    "        with open(self.movies_path, newline='', encoding='ISO-8859-1') as csv_file:\n",
    "                movie_reader = csv.reader(csv_file)\n",
    "                next(movie_reader)  #Skip header line\n",
    "                for row in movie_reader:\n",
    "                    movie_id = int(row[0])\n",
    "                    movie_name = row[1]\n",
    "                    self.movieID_to_name[movie_id] = movie_name\n",
    "                    self.name_to_movieID[movie_name] = movie_id\n",
    "\n",
    "        return ratings_dataset\n",
    "\n",
    "    def get_movie_name(self, movieID):\n",
    "        if movieID in self.movieID_to_name:\n",
    "            return self.movieID_to_name[movieID]\n",
    "        else:\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset\n",
    "Let's find a user that has a similar taste to ours.\n",
    "1. Find a user that watched some movies that we also like.\n",
    "2. Filter these movies by rating 4 or 5.\n",
    "3. Check again if we like these movies.\n",
    "4. We'll use a user that is similar to us to recommend movies and check if the recommendations for him would be good for us too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load ratings dataset\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "potential_test_subject = 2\n",
    "ratings_user_me = ratings[ratings['userId'] == potential_test_subject]\n",
    "\n",
    "# get ratings above 4\n",
    "ratings_user_me = ratings_user_me[ratings_user_me['rating'] >= 4]\n",
    "print('shape:', ratings_user_me.shape)\n",
    "ratings_user_me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load movies dataset\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "print('Ids of selected movies:', ratings_user_me['movieId'].values)\n",
    "\n",
    "names_of_movies_user_me = []\n",
    "for index, row in movies.iterrows():\n",
    "    if row['movieId'] in ratings_user_me['movieId'].values:\n",
    "        names_of_movies_user_me.append(row['title'])\n",
    "\n",
    "print('Amount of movie names:', len(names_of_movies_user_me))\n",
    "names_of_movies_user_me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "On this step we'll build the combination of users and items (movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the ID of the user for whom we want to get recommendations\n",
    "test_subject = '2'\n",
    "\n",
    "# This is the number of movie recommendations we want to generate for the user\n",
    "number_of_recommendations = 10\n",
    "\n",
    "# Load our data set and compute the user similarity matrix\n",
    "# Instantiate the MovieLens object. This object will be used to interact with the MovieLens dataset\n",
    "ml = MovieLens()\n",
    "\n",
    "# Load the latest small dataset from MovieLens. This dataset contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users\n",
    "data = ml.load_movie_lens_latest_small()\n",
    "\n",
    "# Build a training set from the entire data set. This will be used to train our recommendation model\n",
    "train_set = data.build_full_trainset()\n",
    "print(\"Dataset built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "# Options for similarity calculation\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # We are using cosine similarity measure, other options include 'MSD' and 'pearson'\n",
    "    'user_based': True  # We are opting for a user-based collaborative filtering, which means we find users that are similar to the target user and recommend items that those users liked. If this was False, it would be item-based collaborative filtering.\n",
    "}\n",
    "\n",
    "# KNNBasic is a basic collaborative filtering algorithm from the Surprise library\n",
    "model = KNNBasic(sim_options=sim_options)  # Instantiate the model with the similarity options\n",
    "\n",
    "model.fit(train_set)  # Fit the model to the training data. This is where the model learns the item-user relationships in the data.\n",
    "\n",
    "sim_matrix = model.compute_similarities()  # Compute the similarity matrix. This matrix contains the computed similarity measure between each pair of users or items, depending on whether user_based is True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the most similar users to our test subject user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# Get the inner id of the test user from the training set. The inner id is a numeric identifier assigned to the user by the Surprise library.\n",
    "test_user_inner_id = train_set.to_inner_uid(test_subject)\n",
    "\n",
    "# Get the similarity scores of the test user with all other users. The similarity scores are stored in a row of the similarity matrix.\n",
    "similarity_row = sim_matrix[test_user_inner_id]\n",
    "\n",
    "similar_users = []\n",
    "# Enumerate over the similarity scores of the test user with all other users.\n",
    "for inner_id, score in enumerate(similarity_row):\n",
    "    # Exclude the test user from the list of similar users.\n",
    "    if (inner_id != test_user_inner_id):\n",
    "        # Append the inner id and similarity score of each user to the list of similar users.\n",
    "        similar_users.append( (inner_id, score) )\n",
    "\n",
    "# Use a heap data structure to efficiently find the top N users with the highest similarity scores.\n",
    "k_neighbors = heapq.nlargest(number_of_recommendations, similar_users, key=lambda similar_user: similar_user[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a list of candidate items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the candidate items for recommendation along with their cumulative scores\n",
    "candidates = defaultdict(float)\n",
    "\n",
    "# Iterate over the k nearest neighbors of the test user\n",
    "for similarUser in k_neighbors:\n",
    "    inner_id = similarUser[0]  # Get the inner id of the similar user\n",
    "    userSimilarityScore = similarUser[1]  # Get the similarity score between the test user and the similar user\n",
    "\n",
    "    # Get the ratings provided by the similar user\n",
    "    theirRatings = train_set.ur[inner_id]\n",
    "\n",
    "    # Iterate over the ratings provided by the similar user\n",
    "    for ratings in theirRatings:\n",
    "        # For each item rated by the similar user, add up the ratings for each item, weighted by user similarity\n",
    "        # The rating is normalized by dividing by 5.0 (assuming the rating scale is 1-5)\n",
    "        candidates[ratings[0]] += (ratings[1] / 5.0) * userSimilarityScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering step\n",
    "In this case, we discard movies that ware already watched by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of items the user has already seen\n",
    "watched = {}\n",
    "# Iterate over the items and their ratings that the test user has interacted with\n",
    "for itemID, ratings in train_set.ur[test_user_inner_id]:\n",
    "    # Add each item to the dictionary and mark it as seen (1)\n",
    "    watched[itemID] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking step\n",
    "Sort the recommendation candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Get top-rated items from similar users:\n",
    "pos = 0\n",
    "# Sort the candidate items based on their cumulative scores in descending order\n",
    "for itemID, ratingSum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "    # Check if the user has already seen this item\n",
    "    if not itemID in watched:\n",
    "        # Convert the inner id of the item to the raw id. The raw id is the original id of the item in the dataset.\n",
    "        movie_id = train_set.to_raw_iid(itemID)\n",
    "        # Print the name of the movie and its cumulative score\n",
    "        print(ml.get_movie_name(int(movie_id)), ratingSum)\n",
    "        # Increment the position counter\n",
    "        pos += 1\n",
    "        # If we have already printed the top 10 items, break the loop\n",
    "        if (pos > 10):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ia-study-wV_J2Woc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
